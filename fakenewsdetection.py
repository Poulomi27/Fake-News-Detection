# -*- coding: utf-8 -*-
"""FakeNewsDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vn5UQLFq2D1Mhb4pl99R7tK1PG2Pgg05
"""

# Importing libraries
import pandas as pd # Working with "relational" or "labeled" data
import numpy as np
import seaborn as sns #to plot graphs
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re # to determine whether a given text fits a given regular expression,
import string
#A large portion of the data is unstructured and contains text humans can read. Preprocessing that data is necessary.

# Importing the dataset
data_true=pd.read_csv('True.csv')
data_fake=pd.read_csv('Fake.csv')

# The top five rows of the dataframe are shown by default
data_true.head()

data_fake.head()

# Assigning classes to the dataset
data_fake["class"]=0
data_true["class"]=1

# checking the number of rows and columns of datasets
data_true.shape

data_fake.shape

# Manual testing for both the dataset
data_true_manual_testing=data_true.tail(10)
for i in range(21416,21316,-1):
  data_true.drop([i],axis=0,inplace=True)

data_fake_manual_testing=data_true.tail(10)
for i in range(23480,23470,-1):
  data_fake.drop([i],axis=0,inplace=True)

# Assigning classes to the dataset
data_true_manual_testing['class']=1
data_fake_manual_testing['class']=0

#merging the dataset
data_merge=pd.concat([data_fake,data_true],axis=0)
data_merge.head(10)

# As the title, subject and date column will not going to be helpful in identification of the news.
# So, we can drop these column.
data=data_merge.drop(['title','subject','date'],axis=1)

data.isnull().sum()

sns.countplot(data=data,
              x='class',
              order=data['class'].value_counts().index)

from tqdm import tqdm
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud
def preprocess_text(text_data):
	preprocessed_text = []

	for sentence in tqdm(text_data):
		sentence = re.sub(r'[^\w\s]', '', sentence)
		preprocessed_text.append(' '.join(token.lower()
								for token in str(sentence).split()
								if token not in stopwords.words('english')))

	return preprocessed_text
preprocessed_review = preprocess_text(data['text'].values)
data['text'] = preprocessed_review

# Real
consolidated = ' '.join(
	word for word in data['text'][data['class'] == 1].astype(str))
wordCloud = WordCloud(width=1600,
					height=800,
					random_state=21,
					max_font_size=110,
					collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

# Fake
consolidated = ' '.join(
	word for word in data['text'][data['class'] == 0].astype(str))
wordCloud = WordCloud(width=1600,
					height=800,
					random_state=21,
					max_font_size=110,
					collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer


def get_top_n_words(corpus, n=None):
	vec = CountVectorizer().fit(corpus)
	bag_of_words = vec.transform(corpus)
	sum_words = bag_of_words.sum(axis=0)
	words_freq = [(word, sum_words[0, idx])
				for word, idx in vec.vocabulary_.items()]
	words_freq = sorted(words_freq, key=lambda x: x[1],
						reverse=True)
	return words_freq[:n]


common_words = get_top_n_words(data['text'], 20)
df1 = pd.DataFrame(common_words, columns=['Review', 'count'])

df1.groupby('Review').sum()['count'].sort_values(ascending=False).plot(
	kind='bar',
	figsize=(10, 6),
	xlabel="Top Words",
	ylabel="Count",
	title="Bar Chart of Top Words Frequency"
)

#cleaning data
def wordopt(text):
  text=text.lower() #An all-lower case string is produced
  text=re.sub('\[.*?\]', '',text) # All instances of the supplied pattern that match are replaced by the replace string in the returned string.
  text=re.sub("\\W"," ",text)
  text = re.sub("\\W"," ",text)
  text = re.sub('https?://\S+|www\.\S+', '', text)
  text = re.sub('<.*?>+', '', text)
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # returns all available punctuation
  text = re.sub('\n', '', text)
  text = re.sub('\w*\d\w*', '', text)
  return text

# assigning the function wordopt() for every row in the 'text' column
data['text']=data['text'].apply(wordopt)

x=data['text']
y=data['class']

# Defining Training and Testing Data and Splitting Them Into &5 -25 Percent Ratio
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)

from sklearn.model_selection import KFold

kf = KFold(n_splits=9)
for train_index, test_index in kf.split(x.reset_index(drop=True)):
    x_train, x_test = x.iloc[train_index], x.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

#Converting Raw Data Into Matrix for Further Process.
from sklearn.feature_extraction.text import TfidfVectorizer
# The TfidfVectorizer turns a set of raw documents into a TF-IDF feature matrix
vectorization = TfidfVectorizer()
xv_train = vectorization.fit_transform(x_train)
xv_test = vectorization.transform(x_test)
# the fit_transform() is used to train data in order to scale it and learn the scaling parameters

#Logistic Regression Model
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()
LR.fit(xv_train,y_train)

pred_lr=LR.predict(xv_test)

print("Logistic regression Model Accuracy: ",LR.score(xv_test, y_test),"\n")
print("Classification Report: ")
print( classification_report(y_test, pred_lr))

print(accuracy_score(y_test, pred_lr))

from sklearn.tree import DecisionTreeClassifier
DT=DecisionTreeClassifier()
DT.fit(xv_train,y_train)

pred_dt=DT.predict(xv_test)
DT.score(xv_test,y_test)
print(classification_report(y_test,pred_dt))

from sklearn.ensemble import GradientBoostingClassifier
GBC = GradientBoostingClassifier(random_state=0)
GBC.fit(xv_train, y_train)

pred_gbc = GBC.predict(xv_test)
GBC.score(xv_test, y_test)

print(classification_report(y_test, pred_gbc))

from sklearn.ensemble import RandomForestClassifier
RFC = RandomForestClassifier(random_state=0)
RFC.fit(xv_train, y_train)

pred_rfc = RFC.predict(xv_test)
RFC.score(xv_test, y_test)

print(classification_report(y_test, pred_rfc))

def output_lable(n):
    if n == 0:
        return "Fake News"
    elif n == 1:
        return "Not A Fake News"

def manual_testing(news):
    testing_news = {"text":[news]}
    new_def_test = pd.DataFrame(testing_news)
    new_def_test["text"] = new_def_test["text"].apply(wordopt)
    new_x_test = new_def_test["text"]
    new_xv_test = vectorization.transform(new_x_test)
    pred_LR = LR.predict(new_xv_test)
    pred_DT = DT.predict(new_xv_test)
    pred_GBC = GBC.predict(new_xv_test)
    pred_RFC = RFC.predict(new_xv_test)


    return print("\n\nLR Prediction: {} \nDT Prediction: {} \nGBC Prediction: {} \nRFC Prediction: {}".format(output_lable(pred_LR[0]),                                                                                                       output_lable(pred_DT[0]),
                                                                                                              output_lable(pred_GBC[0]),
                                                                                                              output_lable(pred_RFC[0])))

news = str(input())
manual_testing(news)

news1 = str(input())
manual_testing(news1)

news2 = str(input())
manual_testing(news2)